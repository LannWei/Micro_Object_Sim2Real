{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### kernel: lan_1118\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def render_image_at_depth_gpu(A, U, V, lambda_, f_obj, f_eye, lambda_oil, lambda_coverslip, lambda_sample, z, NA, padding_factor, N_obj_sam):\n",
    "    \"\"\"\n",
    "    Simulates the amplitude of an image at a specific depth using GPU acceleration with PyTorch.\n",
    "    \"\"\"\n",
    "    device = A.device  # Ensure computations are done on the same device as input tensor\n",
    "    \n",
    "    # Thin lens propagation transfer function (objective lens)\n",
    "    H_obj = torch.exp(-1j * np.pi * (U**2 + V**2) * lambda_ * z / f_obj)\n",
    "    # Thin lens propagation transfer function (eyepiece)\n",
    "    H_eye = torch.exp(-1j * np.pi * (U**2 + V**2) * lambda_ * z / f_eye)\n",
    "    # Fresnel diffraction transfer function through oil immersion\n",
    "    H_oil = torch.exp(1j * np.pi * lambda_oil * z * (U**2 + V**2))\n",
    "    # Fresnel diffraction transfer function through coverslip\n",
    "    H_coverslip = torch.exp(1j * np.pi * lambda_coverslip * z * (U**2 + V**2))\n",
    "    # Fresnel diffraction transfer function through sample medium\n",
    "    H_sample = torch.exp(1j * np.pi * lambda_sample * z * (U**2 + V**2))\n",
    "    \n",
    "    # Combined transfer function with NA cutoff\n",
    "    H = H_obj * H_eye * H_oil * H_coverslip * H_sample\n",
    "    H[torch.sqrt(U**2 + V**2) > NA / lambda_oil] = 0  # Apply cutoff based on numerical aperture\n",
    "    \n",
    "    # Zero-padding before FFT\n",
    "    padded_size = round(padding_factor * N_obj_sam)\n",
    "    pad_x = (padded_size - N_obj_sam) // 2\n",
    "    pad_y = (padded_size - N_obj_sam) // 2\n",
    "    A_padded = torch.nn.functional.pad(A, (pad_y, pad_y, pad_x, pad_x), mode='constant', value=0)\n",
    "    H_padded = torch.nn.functional.pad(H, (pad_y, pad_y, pad_x, pad_x), mode='constant', value=0)\n",
    "    \n",
    "    # Compute the propagated field\n",
    "    A_fft = torch.fft.fftshift(torch.fft.fft2(torch.fft.fftshift(A_padded)))\n",
    "    B_fft = A_fft * H_padded\n",
    "    B = torch.fft.ifftshift(torch.fft.ifft2(torch.fft.ifftshift(B_fft)))\n",
    "    \n",
    "    # # Verify energy conservation (Parseval's theorem)\n",
    "    # energy_before = torch.sum(torch.abs(A_padded)**2).item()\n",
    "    # energy_after = torch.sum(torch.abs(B)**2).item()\n",
    "    # print(f'Energy before: {energy_before:.6f}, Energy after: {energy_after:.6f}')\n",
    "    \n",
    "    # Crop and accumulate the squared amplitude of each depth slice\n",
    "    crop_start_x = pad_x\n",
    "    crop_end_x = crop_start_x + N_obj_sam\n",
    "    crop_start_y = pad_y\n",
    "    crop_end_y = crop_start_y + N_obj_sam\n",
    "    B_cropped = B[crop_start_x:crop_end_x, crop_start_y:crop_end_y]\n",
    "    amplitude = torch.abs(B_cropped)**2\n",
    "    \n",
    "    return amplitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgbFourBall0000\n",
      "rgbFourBall0010\n",
      "rgbFourBall0020\n",
      "rgbFourBall0030\n",
      "rgbFourBall0040\n",
      "rgbFourBall0050\n",
      "rgbFourBall0060\n",
      "rgbFourBall0070\n",
      "rgbFourBall1010\n",
      "rgbFourBall1020\n",
      "rgbFourBall1030\n",
      "rgbFourBall1040\n",
      "rgbFourBall1050\n",
      "rgbFourBall1060\n",
      "rgbFourBall1070\n",
      "rgbFourBall2020\n",
      "rgbFourBall2030\n",
      "rgbFourBall2040\n",
      "rgbFourBall2050\n",
      "rgbFourBall2060\n",
      "rgbFourBall2070\n",
      "rgbFourBall3030\n",
      "rgbFourBall3040\n",
      "rgbFourBall3050\n",
      "rgbFourBall3060\n",
      "rgbFourBall3070\n",
      "rgbFourBall4040\n",
      "rgbFourBall4050\n",
      "rgbFourBall4060\n",
      "rgbFourBall4070\n",
      "rgbFourBall5050\n",
      "rgbFourBall5060\n",
      "rgbFourBall5070\n",
      "rgbFourBall6060\n",
      "rgbFourBall6070\n",
      "rgbFourBall7070\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.color import rgb2gray\n",
    "from torch import tensor\n",
    "from skimage.util import img_as_ubyte\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set new size for the cropped images\n",
    "crop_size = 500  # Variable for cropping size; smaller sizes increase rendering speed.\n",
    "# Define ranges\n",
    "start = torch.linspace(0, 1e-5, 20)\n",
    "seg1 = torch.linspace(1e-5, -1e-5, 40)\n",
    "seg2 = torch.linspace(-1e-5, 1e-5, 40)\n",
    "seg3 = torch.linspace(1e-5, -1e-5, 40)\n",
    "seg4 = torch.linspace(-1e-5, 1e-5, 40)\n",
    "seg5 = torch.linspace(1e-5, -1e-5, 40)\n",
    "seg6 = torch.linspace(-1e-5, 1e-5, 40)\n",
    "seg7 = torch.linspace(1e-5, -1e-5, 40)\n",
    "seg8 = torch.linspace(-1e-5, 1e-5, 40)\n",
    "seg9 = torch.linspace(1e-5, -1e-5, 40)\n",
    "seg10 = torch.linspace(-1e-5, 1e-5, 40)\n",
    "seg11 = torch.linspace(1e-5, -1e-5, 40)\n",
    "seg12 = torch.linspace(-1e-5, 1e-5, 40)\n",
    "seg13 = torch.linspace(1e-5, -1e-5, 40)\n",
    "seg14 = torch.linspace(-1e-5, 1e-5, 40)\n",
    "end = torch.linspace(1e-5, 0, 20)\n",
    "# Concatenate the segments\n",
    "dz = torch.cat((start, seg1, seg2, seg3, seg4, seg5, seg6, seg7, seg8, seg9, seg10, seg11, seg12, seg13, seg14, end))\n",
    "\n",
    "# Precompute some common parameters\n",
    "lambda_ = 632.8e-6  # mm\n",
    "NA = 1.45  # Numerical Aperture for oil immersion\n",
    "f_obj = 50e-3  # mm, objective focal length\n",
    "f_eye = 20e-3  # mm, eyepiece focal length\n",
    "M = 100  # Total magnification\n",
    "pixel_size = 6.5e-3  # mm\n",
    "N_obj_sam = crop_size\n",
    "padding_factor = 2\n",
    "plotW = 4  # mm\n",
    "n_oil = 1.515  # Refractive index for immersion oil\n",
    "n_coverslip = 1.515  # Refractive index for coverslip\n",
    "n_sample = 1.33  # Refractive index for sample medium\n",
    "lambda_oil = lambda_ / n_oil\n",
    "lambda_coverslip = lambda_ / n_coverslip\n",
    "lambda_sample = lambda_ / n_sample\n",
    "dx = pixel_size / M\n",
    "du = 1 / (N_obj_sam * dx)\n",
    "\n",
    "# Frequency grid\n",
    "u = torch.linspace(-N_obj_sam / 2, N_obj_sam / 2 - 1, N_obj_sam) * du\n",
    "U, V = torch.meshgrid(u, u, indexing='ij')  # indexing='ij' for MATLAB-like behavior\n",
    "U = U.to('cuda')  # Use GPU\n",
    "V = V.to('cuda')  # Use GPU\n",
    "\n",
    "# Get folder list\n",
    "folder_list = [f for f in os.listdir() if os.path.isdir(f) and f.startswith(\"rgb\")]\n",
    "\n",
    "center_crop = transforms.CenterCrop(350)\n",
    "\n",
    "\n",
    "for folder_name in folder_list:\n",
    "    print(folder_name)\n",
    "    \n",
    "    # Get image files in folder\n",
    "    image_files = [f for f in os.listdir(folder_name) if f.startswith(\"rgb_image\") and f.endswith(\".png\")]\n",
    "    num_images = len(image_files)\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # Load and preprocess the image\n",
    "        image_path = os.path.join(folder_name, image_file)\n",
    "        A = imread(image_path)\n",
    "        A = rgb2gray(A)\n",
    "        A = torch.tensor(A, dtype=torch.float32, device='cuda')\n",
    "\n",
    "        # Crop the image to the center\n",
    "        height, width = A.shape\n",
    "        start_row = (height - crop_size) // 2\n",
    "        start_col = (width - crop_size) // 2\n",
    "        A = A[start_row:start_row + crop_size, start_col:start_col + crop_size]\n",
    "\n",
    "        # Initialize final_image\n",
    "        final_image = torch.zeros((N_obj_sam, N_obj_sam), device='cuda')\n",
    "\n",
    "        idx = 1\n",
    "        for z in dz:\n",
    "            amplitude = render_image_at_depth_gpu(A, U, V, lambda_, f_obj, f_eye, lambda_oil, lambda_coverslip, lambda_sample, z, NA, padding_factor, N_obj_sam)\n",
    "\n",
    "            # Accumulate energy\n",
    "            final_image += amplitude\n",
    "\n",
    "            # Standardize and save each depth slice amplitude\n",
    "            amplitude_standardized = (amplitude - amplitude.min()) / (amplitude.max() - amplitude.min())\n",
    "            amplitude_standardized = center_crop(amplitude_standardized)\n",
    "            amplitude_standardized_uint8 = img_as_ubyte(amplitude_standardized.cpu().numpy())  # Convert to uint8\n",
    "            # imsave(os.path.join(output_folder, f\"amplitude_depth_{z.item():.6f}.png\"), amplitude_standardized_uint8)\n",
    "\n",
    "            part1 = folder_name[-4:-2]  # Extract \"P10\"\n",
    "            part2 = folder_name[-2:]    # Extract \"R20\"\n",
    "            type_name = f\"P{part1}_R{part2}\"\n",
    "            output_folder = os.path.join(\"Output\", type_name)\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            imsave(os.path.join(output_folder, f\"Image_{idx}.png\"), amplitude_standardized_uint8)\n",
    "\n",
    "            idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rgbFourBall6070/output'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lan_GAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
